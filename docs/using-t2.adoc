// Header of this document:

= Using T2
:toc:
:toc-placement: preamble
:toclevels: 3
:showtitle:
:base-repo: https://github.com/stackabletech/t2
:imagesdir: diagrams

// Need some preamble to get TOC:
{empty}

At Stackable, we have a https://t2.stackable.tech/swagger-ui/[running instance of T2, window="_blank"] to provision our own clusters. As described in the link:../README.adoc[README], we use it for integration testing and troubleshooting and do not provide access publicly (except DIY, see below).

== Create a cluster

=== Create a cluster in T2's infrastructure

With request to https://t2.stackable.tech/swagger-ui/#/cluster-controller/createClusterUsingPOST[this endpoint, window="_blank"] you can create a new cluster. You have to provide the Stackable cluster definition as payload and a Token via the `t2-token` HTTP header to authenticate. The response is a description of the current cluster state.

To track the progress while the cluster is created, you can use https://t2.stackable.tech/swagger-ui/#/cluster-controller/getClusterUsingGET[this endpoint, window="_blank"]. The desired state you want to wait for is `RUNNING`. (requires token as well)

To see in more detail what T2 is doing to create your cluster, you can https://t2.stackable.tech/swagger-ui/#/cluster-controller/getLogUsingGET[trace the output log here, window="_blank"]. (requires token as well)

Once the cluster is up and running, you can download a script https://t2.stackable.tech/swagger-ui/#/cluster-controller/getClientScriptUsingGET[here, window="_blank"] which provides you with a convenient way to access the nodes of the cluster (see <<client_script>> below).

Another way of accessing the cluster is using a prepared SSH config together with one of the private keys corresponding to a public key in the cluster definition file. You can download the SSH config https://t2.stackable.tech/swagger-ui/#/cluster-controller/getSshConfigUsingGET[here, window="_blank"]. When you use the SSH config or copy parts of it into your own SSH config, you can use tools like `ssh` or `scp` to interact with the cluster.

To check which versions of the Stackable operators are running in your cluster, you can get an overview https://t2.stackable.tech/swagger-ui/#/cluster-controller/getLogUsingGET[here, window="_blank"]. (requires token as well)

==== VPN (Wireguard)

A very convenient way to work with a running cluster is to put your machine in the provided VPN. Using http://t2.stackable.tech/swagger-ui/#/cluster-controller/getWireguardConfigUsingGET[this endpoint, window="_blank"] you can download a Wireguard config file which is ready to be used. Please refer to the Wireguard docs for your system to find out how to use this.

Once you are in the same network with the cluster, you can http://t2.stackable.tech/swagger-ui/#/cluster-controller/getKubeconfigUsingGET[download a kubeconfig file, window="_blank"] to interact with the Kubernetes system via *kubectl*, *k9s* or the like.



=== Create a cluster in your infrastructure

To create a cluster yourself, we offer what we call the *DIY option*. To use it, you can use https://t2.stackable.tech/swagger-ui/#/diy-cluster-controller/createClusterUsingGET[this service]. You have to provide a Stackable cluster definition as request body and get a ZIP file in return. This ZIP file comes with a `readme.txt` which explains the usage.


[[client_script]]
== The Stackable client script

This script can be used to access a running cluster conveniently. 

The script expects the private SSH key (matching one of the public keys in the Stackable cluster definition, see <<yaml>>) to be in your keystore (`~/.ssh/` in Linux). If you keep it at another location, you can provide the path to the private key with the `-i` option.

To ssh into a host, just provide the hostname as the single parameter, e.g.

[source,bash]
----
./stackable.sh worker-12
----

or, with path to key: 

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key
----

If you want to execute a command on the host, you can add it as a second param, e.g.

[source,bash]
----
./stackable.sh worker-12 "kubectl get nodes"
----

or, with path to key:

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key "kubectl get nodes"
----

[[yaml]]
== The Stackable cluster definition YAML

In either way you choose to use T2, you have to provide a definition of the Stackable cluster you want to create. You do this in a single YAML file. This section describes the cluster definition.

This is an example cluster definition, please find detailed specification of the structure below.

*Warning!* This exact config cannot be copy/pasted and used, as it is kind of a superset of options and therefore may contain contradicting and/or deprecated values. Please refer to the reference table below if you plan to setup a cluster.

[source,yaml]
----
apiVersion: t2.stackable.tech/v1
kind: Infra
template: ionos-debian-10
metadata: 
  name: stackable-demo
  description: "This is the cluster I want!"
domain: stackable.demo
publicKeys:
  - "ssh-rsa AAAAB3NzaC1..."
  - "ssh-rsa AAAACmtp4Ko..."
spec:
  region: de/fra
  cpuFamily: INTEL_XEON
  wireguard: true
  k8sVersion: "1.21.6"
  versions:
    spark-operator: "0.2.0"
    kafka-operator: "0.3.0"
    zookeper-operator: "NIGHTLY"
    monitoring-operator: "RELEASE"
  orchestrator:
    numberOfCores: 4
    memoryMb: 8192
    diskType: HDD
    diskSizeGb: 15
  nodes:
    main:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    worker:
      numberOfNodes: 4
      numberOfCores: 4
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    testdriver:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 2048
      diskType: HDD 
      diskSizeGb: 15
      k8s_node: false
services:
  spark-primary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-primary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9999
            masterWebUiPort: 11111
      worker:
        selectors:
          - nodeName: "worker-1.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-3.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0
  spark-secondary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-secondary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9998
            masterWebUiPort: 11112
      worker:
        selectors:
          - nodeName: "worker-2.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-4.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0        
----

The following sections describe the fields of the cluster definition in more detail:

=== Templates

The most important choice you have to make in a cluster definition is right in line #3: the template. There are two kinds of templates: 

* "K3s-based self-provisioned": We use the compute power of the cloud provider and install a K3s-based Kubernetes cluster ourselves.
* "Managed K8s": We use the managed Kubernetes products of the cloud provider.

The following table lists the currently available templates:

[options="header"]
|=======
|Type|key |Cloud provider| Linux distribution
.7+|K3s |ionos-centos-7 | IONOS Cloud | CentOS 7
|ionos-centos-8 | IONOS Cloud | CentOS 8
|ionos-debian-10 | IONOS Cloud | Debian 10
|aws-centos-8 | Amazon EC2 | CentOS 8
|hcloud-centos-8 | Hetzner Cloud | CentOS 8
|hcloud-debian-10 | Hetzner Cloud | Debian 10
|pluscloud-open-centos-8 | PlusCloud Open (Plusserver, SCS implementation, based on OpenStack) | CentOS 8
.2+|managed K8s |azure-aks | Microsoft Azure | 
|aws-eks | Amazon AWS | 
|=======

=== Configuration options

To keep the following informations somewhat handy, they are split in the options for K3s-based clusters and managed K8s

==== Configuration (K3s)

[options="header"]
|=======
|key |description |IONOS Cloud |Amazon EC2| PlusCloud Open| Hetzner Cloud
|apiVersion |always `t2.stackable.tech/v1` 4+| 
|kind |always `Infra` 4+| 
|template |see section above 4+| 
|metadata.name |name of the cluster 4+| 
|metadata.description |description of the cluster 4+| 
|domain |domain for DNS inside the cluster or when accessing through VPN 4+| 
|publicKeys |list of SSH public keys to allow access to cluster nodes 4+| 
|spec.region |one of the regions that the cloud vendor provides | e.g. `de/fra`, `de/txl` | e.g. `eu-central-1` 2+| not available
|spec.location |one of the locations that the cloud vendor provides 3+| not available | e.g. `nbg1`, `hel1`
|spec.cpuFamily |(optional) specify CPU-Family for all servers. The allowed values depend on the datacenter location you set up your cluster in. Please refer to your IONOS account for information about available CPUs and default values. | e.g. `INTEL_XEON` 3+| not available
|spec.wireguard |(boolean, optional, defaults to `false`) Should a wireguard server be started on the bastion host? Leaving wireguard switched off when you don't need it speeds up the start of the cluster. | | not available | not available |
|spec.k8sVersion |The K8s version (optional, defaults to whatever the K3s installation script regards as the default), see https://github.com/k3s-io/k3s/releases[here, window="_blank"] which versions are available. 4+| 
|spec.versions |(optional) Map of versions of the Stackable components to be used in this cluster. See below for a list of Stackable components. 4+|
|spec.orchestrator |(optional) The orchestrator node is the Stackable node which hosts the operators. It is required, you cannot opt out of having one. It has reasonable defaults, but you can overwrite them with the config properties in this section. Be cautious not to configure an orchestrator which has too little power. See following entries for details. 4+|
|spec.orchestrator.numberOfCores |(optional) # of cores the orchestrator should have | default: `4` 3+| not available
|spec.orchestrator.memoryMb |(optional) amount of memory the orchestrator should have in MB | default: `8192` 3+| not available 
|spec.orchestrator.diskType |(optional) type of disk the orchestrator should have | default: `HDD` | default: `gp2` (general purpose SSD), see links below 2+| not available 
|spec.orchestrator.diskSizeGb |(optional) size of the disk of the orchestrator in GB | default: `50` | default: `50` 2+| not available 
|spec.orchestrator.awsInstanceType |(optional) AWS EC2 instance type | not available | default: `t2.xlarge`, see links below 2+| not available
|spec.orchestrator.openstackFlavorName |(optional) 'Flavor' of the instance in OpenStack 2+| not available | defaults to `8C-16GB-60GB` | not available
|spec.orchestrator.serverType |(optional) ServerType of the node in Hetzner Cloud 3+| not available | defaults to `cx41` 
|spec.nodes |map of node types with their specification 4+| 
|spec.nodes.<type>.numberOfNodes |# of nodes of the given type 4+|
|spec.nodes.<type>.numberOfCores |# of cores each node of the given type should have | e.g. `8` 3+| not available
|spec.nodes.<type>.memoryMb |amount of memory each node of the given type should have in MB | e.g. `8192` 3+| not available
|spec.nodes.<type>.diskType | type of disk each node of the given type should have | e.g.: `SSD` | default: `gp2` (general purpose SSD), see links below 2+| not available
|spec.nodes.<type>.diskSizeGb |size of the disk of the given node in GB | e.g. `500` | default: `50` 2+| not available
|spec.nodes.<type>.awsInstanceType |(optional) AWS EC2 instance type | not available | default: `t2.medium`, see links below 2+| not available
|spec.nodes.<type>.openstackFlavorName |(optional) 'Flavor' of the instance in OpenStack 2+| not available | defaults to `2C-4GB-20GB` | not available
|spec.nodes.<type>.serverType |(optional) ServerType of the node in Hetzner Cloud 3+| not available | defaults to `cx21`
|spec.nodes.<type>.k8s_node |(boolean, optional, defaults to `true`) Should this node work as a Kubernetes node? 4+|
|services |Map of service descriptions as embedded YAMLs. See below for available services. 4+|
|=======

==== Configuration (managed K8s)

[options="header"]
|=======
|key |description |Azure AKS|Amazon EKS
|apiVersion |always `t2.stackable.tech/v1` 2+| 
|kind |always `Infra` 2+| 
|template |see section above 2+| 
|metadata.name |name of the cluster 2+| 
|metadata.description |description of the cluster 2+| 
|spec.location |one of the locations that the cloud vendor provides | e.g. `West Europe` | not available
|spec.region |one of the regions that the cloud vendor provides | not available | e.g. `eu-central-1` 
|spec.k8sVersion |The K8s version (optional, defaults to whatever the cloud provider regards as the default) 2+| 
|spec.node_count |# of nodes in the cluster 2+|
|spec.vm_size |one of the VM sizes that the cloud vendor provides (optional) | e.g. `Standard_D2_v2` (which is also the default) | not available
|spec.versions |(optional) Map of versions of the Stackable components to be used in this cluster. See below for a list of Stackable components. 2+|
|services |Map of service descriptions as embedded YAMLs. See below for available services. 2+|
|=======

==== Links

* https://aws.amazon.com/de/ec2/instance-types/[AWS EC2 instance types]
* https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html[AWS EC2 volume types]
* https://www.hetzner.com/de/cloud[Hetzner Cloud server types]


=== Stackable components

These are components that a Stackable cluster is made of. You can specify their versions with the `spec.versions` section in the cluster definition (see above)

[options="header"]
|=======
|Name |key
|https://github.com/stackabletech/druid-operator[Stackable Operator for Apache Druid ] |`druid-operator`
|https://github.com/stackabletech/hbase-operator[Stackable Operator for Apache HBase ] |`hbase-operator`
|https://github.com/stackabletech/hdfs-operator[Stackable Operator for Apache HDFS] |`hdfs-operator`
|https://github.com/stackabletech/hive-operator[Stackable Operator for Apache Hive] |`hive-operator`
|https://github.com/stackabletech/kafka-operator[Stackable Operator for Apache Kafka] |`kafka-operator`
|https://github.com/stackabletech/monitoring-operator[Stackable Operator for Monitoring and Metrics] |`monitoring-operator`
|https://github.com/stackabletech/nifi-operator[Stackable Operator for Apache NiFi] |`nifi-operator`
|https://github.com/stackabletech/opa-operator[Stackable Operator for OpenPolicyAgent (OPA)] |`opa-operator`
|https://github.com/stackabletech/regorule-operator[Stackable RegoRule Operator] |`regorule-operator`
|https://github.com/stackabletech/spark-operator[Stackable Operator for Apache Spark] |`spark-operator`
|https://github.com/stackabletech/superset-operator[Stackable Operator for Apache Superset] |`superset-operator`
|https://github.com/stackabletech/trino-operator[Stackable Operator for Trino] |`trino-operator`
|https://github.com/stackabletech/zookeeper-operator[Stackable Operator for Apache ZooKeeper] |`zookeeper-operator`
|=======


==== Version literals

As shown in the example `cluster.yaml`, you can specify the versions of the Stackable components in the `spec.versions` section. The following table shows the different ways to do so by example:

[options="header"]
|=======
|example |description
|`RELEASE` | The newest release version which can be found in the Stackable repository
| (no version specified) | same as `RELEASE`
|`NIGHTLY` | The newest nightly version which can be found in the Stackable repository
|`0.2.0-mr404` | latest build of version 0.2.0 from GitHub Pull Request #404
|`0.3.0-nightly` | latest nightly build of version 0.3.0
|`0.6.1` | realeased version 0.6.1
|=======


=== Service descriptions

The service descriptions depend on the used services. Please refer to the documentation of the operator for the product. You find the links to the components in the table above.

== Docker image to use T2

We provide the Docker image `docker.stackable.tech/t2-testdriver` to make the usage of T2 in CI pipelines and the like easier.

The Container does the following:

* Launch a cluster according to your cluster definition (if no dry run)
* Execute the provided test script and record its output
* Tear down the cluster after the test script has terminated

=== Input

[options="header"]
|=======
|Feature |How to use |Description
|Target directory | Map as volume to `/target/` | The target directory for the output
|Cluster definition | Map as file to `/cluster.yaml` | The cluster definition as described above
|Test script | Map as file to `/test.sh` | The script containing the test to be run once the cluster is up and running
|T2 URL | mandatory as environment variable `T2_URL` | The URL of T2 to use
|T2 Token | mandatory as environment variable `T2_TOKEN` | The token to access T2
|User/Group for target directory | optional as environment variable `UID_GID` (format `<UID>:<GID>`), defaults to `0:0` (root) | All stuff which is written into the target dir is owned by this user/group combination.
|Dry run | optional as environment variable `DRY_RUN`, defaults to `false` | If set to `true`, the container does not create a cluster but starts with the test script right away.
|Interactive mode | optional as environment variable `INTERACTIVE_MODE`, defaults to `false` | If set to `true`, the container does halt after the creation of the cluster and does *not* execute the test script. 
The idea is that you can execute whatever you like interactively by `docker exec`-ing into the container. To allow the container to shut down the cluster properly, you should create a file `/cluster_lock` to notify the container to go ahead and terminate the cluster, otherwise you may end up with a dangling (and expensive) cluster...
|=======

=== Output files

The following files are created in the directory mounted into `/target/`:

[options="header"]
|=======
|File |Description
|`test_output.log` |Output of the test script
|`testdriver.log` | Log file of the testdriver container itself
|`stackable-versions.txt` | Text file containing the versions of the installed Stackable components in the cluster
|=======

=== Return code

The return code of the Docker container process is the return code of the test script!


