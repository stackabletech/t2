// Header of this document:

= Using T2
:toc:
:toc-placement: preamble
:toclevels: 3
:showtitle:
:base-repo: https://github.com/stackabletech/t2
:imagesdir: diagrams

// Need some preamble to get TOC:
{empty}

At Stackable, we have a https://t2.stackable.tech/swagger-ui/[running instance of T2, window="_blank"] to provision our own clusters. As described in the link:../README.adoc[README], we use it for integration testing and troubleshooting and do not provide access publicly (except DIY, see below).

== Create a cluster

=== Create a cluster using the REST API.

To create a cluster with T2, you can use the REST API which is fully documented https://t2.stackable.tech/swagger-ui/index.html#/[here, window="_blank"]. This section here gives you a small example of the most common steps you take when working with a cluster managed by T2. The example requests are written in the format of the https://github.com/Huachao/vscode-restclient[REST client plugin, window="_blank"] for Visual Studio Code.

==== Step 1: Launch the cluster

With this request, you start the creation of a cluster:
[source,yaml]
----
POST https://t2.stackable.tech/api/clusters
t2-token: (put secret token here)
Content-Type: application/yaml

---
apiVersion: "t2.stackable.tech/v1"
kind: "Infra"
template: gke
metadata:
  name: "gke-test"
  description: "Test cluster in GKE"
spec:
  region: europe-central2
  k8sVersion: 1.24
  node_count: 3
  machineType: e2-standard-4
  versions:
    _-operator: NIGHTLY
----

Please refer to the <<cluster_definition>> section to see which options you have to specify the cluster.

As a response, you get the metadata of the cluster, which contains the current state and a history of its states since creation:
[source,json]
----
{
  "id": "2b1e053c-ffe1-45ac-91b2-67f73d766c2a",
  "status": {
    "failed": false,
    "state": "TERRAFORM_APPLY"
  },
  "dateTimeCreated": "2022-10-05T06:11:27.234051",
  "history": [
    {
      "status": {
        "failed": false,
        "state": "NEW"
      },
      "timestamp": "2022-10-05T06:11:27.23406",
      "timeSinceClusterLaunch": "PT0.000009S",
      "description": null
...      
----

You can see that the cluster is not yet ready for use, but currently applies a Terraform description.

==== Step 2: Check the cluster state

From the response in step 1, you have to copy the cluster's ID, which lets you access the cluster as a resource using the T2 API. To query the current state of the cluster, use a request like this:
[source,yaml]
----
GET https://t2.stackable.tech/api/clusters/2b1e053c-ffe1-45ac-91b2-67f73d766c2a
t2-token: (put secret token here)
----

Eventually, you will get the response which confirms that your cluster is `RUNNING` and therefore ready for use:
[source,json]
----
{
  "id": "2b1e053c-ffe1-45ac-91b2-67f73d766c2a",
  "status": {
    "failed": false,
    "state": "RUNNING"
  },
  "dateTimeCreated": "2022-10-05T06:11:27.234051",
  "history": [
    {
      "status": {
        "failed": false,
        "state": "NEW"
      },
      "timestamp": "2022-10-05T06:11:27.23406",
      "timeSinceClusterLaunch": "PT0.000009S",
      "description": null
    },
...      
----

==== Step 3: Get the cluster access file

T2 offers clusters from 5 different cloud vendors. As the way to access a cluster differs from vendor to vendor, you have to get the so called "cluster access file" which contains everything you need to access the cluster, including instructions how to use it:
[source,yaml]
----
GET https://t2.stackable.tech/api/clusters/2b1e053c-ffe1-45ac-91b2-67f73d766c2a/access
t2-token: (put secret token here)
----

The access file should be self-explanatory. There are basically two ways to access a cluster:

* Some clusters offer a static `kubeconfig` file which is contained in the access file. This can be used to access the cluster.
* Some vendors (currently Google/GKE and AWS/EKS) require a login with a user/principal of the cloud platform. In these cases, the access file contains the credentials of a temporary user along with the instructions how to log in and create the `kubeconfig`.

==== Step 4: Delete the cluster

Once you are done with whatever you were up to with the cluster, please remember to shut the cluster down (or "delete the resource" in REST terms):
[source,yaml]
----
DELETE https://t2.stackable.tech/api/clusters/2b1e053c-ffe1-45ac-91b2-67f73d766c2a
t2-token: (put secret token here)
----

The deletion starts and you can check the cluster's state using the request from step #2 until the cluster is `TERMINATED`:
[source,json]
----
{
  "id": "2b1e053c-ffe1-45ac-91b2-67f73d766c2a",
  "status": {
    "failed": false,
    "state": "TERMINATED"
  },
  "dateTimeCreated": "2022-10-05T06:11:27.234051",
  "history": [
    {
      "status": {
        "failed": false,
        "state": "NEW"
      },
      "timestamp": "2022-10-05T06:11:27.23406",
      "timeSinceClusterLaunch": "PT0.000009S",
      "description": null
    },
...    
----

=== Create a cluster in your infrastructure (deprecated)

To create a cluster yourself, we offer what we call the *DIY option*. To use it, you can use https://t2.stackable.tech/swagger-ui/#/diy-cluster-controller/createClusterUsingGET[this service]. You have to provide a Stackable cluster definition as request body and get a ZIP file in return. This ZIP file comes with a `readme.txt` which explains the usage.


[[client_script]]
== The Stackable client script (deprecated)

This script can be used to access a running cluster conveniently. 

The script expects the private SSH key (matching one of the public keys in the Stackable cluster definition, see <<cluster_definition>>) to be in your keystore (`~/.ssh/` in Linux). If you keep it at another location, you can provide the path to the private key with the `-i` option.

To ssh into a host, just provide the hostname as the single parameter, e.g.

[source,bash]
----
./stackable.sh worker-12
----

or, with path to key: 

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key
----

If you want to execute a command on the host, you can add it as a second param, e.g.

[source,bash]
----
./stackable.sh worker-12 "kubectl get nodes"
----

or, with path to key:

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key "kubectl get nodes"
----

[[cluster_definition]]
== The Stackable cluster definition YAML

In either way you choose to use T2, you have to provide a definition of the Stackable cluster you want to create. You do this in a single YAML file. This section describes the cluster definition.

This is an example cluster definition, please find detailed specification of the structure below.

*Warning!* This exact config cannot be copy/pasted and used, as it is kind of a superset of options and therefore may contain contradicting and/or deprecated values. Please refer to the reference table below if you plan to setup a cluster.

[source,yaml]
----
apiVersion: t2.stackable.tech/v1
kind: Infra
template: ionos-debian-10
metadata: 
  name: stackable-demo
  description: "This is the cluster I want!"
domain: stackable.demo
publicKeys:
  - "ssh-rsa AAAAB3NzaC1..."
  - "ssh-rsa AAAACmtp4Ko..."
spec:
  region: de/fra
  cpuFamily: INTEL_XEON
  wireguard: true
  k8sVersion: "v1.23"
  versions:
    spark-operator: "0.2.0"
    kafka-operator: "0.3.0"
    zookeper-operator: "NIGHTLY"
    monitoring-operator: "RELEASE"
  orchestrator:
    numberOfCores: 4
    memoryMb: 8192
    diskType: HDD
    diskSizeGb: 15
  nodes:
    main:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    worker:
      numberOfNodes: 4
      numberOfCores: 4
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    testdriver:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 2048
      diskType: HDD 
      diskSizeGb: 15
      k8s_node: false
services:
  spark-primary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-primary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9999
            masterWebUiPort: 11111
      worker:
        selectors:
          - nodeName: "worker-1.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-3.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0
  spark-secondary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-secondary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9998
            masterWebUiPort: 11112
      worker:
        selectors:
          - nodeName: "worker-2.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-4.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0        
----

The following sections describe the fields of the cluster definition in more detail:

=== Templates

The most important choice you have to make in a cluster definition is right in line #3: the template. There are two kinds of templates: 

* "K3s-based self-provisioned": We use the compute power of the cloud provider and install a K3s-based Kubernetes cluster ourselves.
* "Managed K8s": We use the managed Kubernetes products of the cloud provider.

The following table lists the currently available templates:

[options="header"]
|=======
|Type|key |Cloud provider| Linux distribution
.9+|K3s |ionos-centos-8 | IONOS Cloud | CentOS 8
|ionos-debian-10ยน | IONOS Cloud | Debian 10
|ionos-debian-11 | IONOS Cloud | Debian 11
|aws-centos-8ยน | Amazon EC2 | CentOS 8
|hcloud-centos-8 | Hetzner Cloud | CentOS 8
|hcloud-centos-9 | Hetzner Cloud | CentOS 9
|hcloud-debian-10 | Hetzner Cloud | Debian 10
|hcloud-debian-11 | Hetzner Cloud | Debian 11
|pluscloud-open-centos-8ยน | PlusCloud Open (Plusserver, SCS implementation, based on OpenStack) | CentOS 8
.4+|managed K8s |azure-aks | Microsoft Azure | 
|aws-eks | Amazon AWS | 
|ionos-k8s | IONOS Cloud |
|gke | Google Kubernetes Engine |
|=======

ยนtemporarily disabled

=== Configuration options

To keep the following informations somewhat handy, they are split in the options for K3s-based clusters and managed K8s

==== Configuration (K3s)

[options="header"]
|=======
|key |description |IONOS Cloud |Amazon EC2| PlusCloud Open| Hetzner Cloud
|apiVersion |always `t2.stackable.tech/v1` 4+| 
|kind |always `Infra` 4+| 
|template |see section above 4+| 
|metadata.name |name of the cluster 4+| 
|metadata.description |description of the cluster 4+| 
|domain |domain for DNS inside the cluster or when accessing through VPN 4+| 
|publicKeys |list of SSH public keys to allow access to cluster nodes 4+| 
|spec.region |one of the regions that the cloud vendor provides | e.g. `de/fra`, `de/txl` | e.g. `eu-central-1` 2+| not available
|spec.location |one of the locations that the cloud vendor provides 3+| not available | HCloud datacenter location, e.g. `fsn1`, `nbg1`, `hel1`. If omitted (recommended and default), one location in central Europe is selected.
|spec.cpuFamily |(optional) specify CPU-Family for all servers. The allowed values depend on the datacenter location you set up your cluster in. Please refer to your IONOS account for information about available CPUs and default values. | e.g. `INTEL_XEON` 3+| not available
|spec.wireguard |(boolean, optional, defaults to `false`) Should a wireguard server be started on the bastion host? Leaving wireguard switched off when you don't need it speeds up the start of the cluster. | | not available | not available |
|spec.k8sVersion |The K3s release to be installed. K3s offers a channel for each minor version of K8s, the channels are named `v1.21`, `v1.22` etc. Special channels are `stable` and `latest`. `stable` is the default for T2. See https://update.k3s.io/v1-release/channels[here, window="_blank"] to inspect which versions are the latest versions of each channel. 4+| 
|spec.versions |(optional) Map of versions of the Stackable operators to be used in this cluster. See below for a list of Stackable components. 4+|
|spec.orchestrator |(optional) The orchestrator node is the Stackable node which hosts the operators. It is required, you cannot opt out of having one. It has reasonable defaults, but you can overwrite them with the config properties in this section. Be cautious not to configure an orchestrator which has too little power. See following entries for details. 4+|
|spec.orchestrator.numberOfCores |(optional) # of cores the orchestrator should have | default: `4` 3+| not available
|spec.orchestrator.memoryMb |(optional) amount of memory the orchestrator should have in MB | default: `8192` 3+| not available 
|spec.orchestrator.diskType |(optional) type of disk the orchestrator should have | default: `HDD` | default: `gp2` (general purpose SSD), see links below 2+| not available 
|spec.orchestrator.diskSizeGb |(optional) size of the disk of the orchestrator in GB | default: `50` | default: `50` 2+| not available 
|spec.orchestrator.awsInstanceType |(optional) AWS EC2 instance type | not available | default: `t2.xlarge`, see links below 2+| not available
|spec.orchestrator.openstackFlavorName |(optional) 'Flavor' of the instance in OpenStack 2+| not available | defaults to `8C-16GB-60GB` | not available
|spec.orchestrator.serverType |(optional) ServerType of the node in Hetzner Cloud 3+| not available | defaults to `cpx41` 
|spec.nodes |map of node types with their specification 4+| 
|spec.nodes.<type>.numberOfNodes |# of nodes of the given type 4+|
|spec.nodes.<type>.numberOfCores |# of cores each node of the given type should have | default: `4` 3+| not available
|spec.nodes.<type>.memoryMb |amount of memory each node of the given type should have in MB | default `4096` 3+| not available
|spec.nodes.<type>.diskType | type of disk each node of the given type should have | default: `SSD` | default: `gp2` (general purpose SSD), see links below 2+| not available
|spec.nodes.<type>.diskSizeGb |size of the disk of the given node in GB | default: `500` | default: `50` 2+| not available
|spec.nodes.<type>.awsInstanceType |(optional) AWS EC2 instance type | not available | default: `t2.medium`, see links below 2+| not available
|spec.nodes.<type>.openstackFlavorName |(optional) 'Flavor' of the instance in OpenStack 2+| not available | defaults to `2C-4GB-20GB` | not available
|spec.nodes.<type>.serverType |(optional) ServerType of the node in Hetzner Cloud 3+| not available | defaults to `cpx21`
|spec.nodes.<type>.k8s_node |(boolean, optional, defaults to `true`) Should this node work as a Kubernetes node? 4+|
|services |Map of service descriptions as embedded YAMLs. See below for available services. 4+|
|=======

==== Configuration (managed K8s)

[options="header"]
|=======
|key |description |Azure AKS|Amazon EKS |IONOS K8S | Google GKE
|apiVersion |always `t2.stackable.tech/v1` 4+| 
|kind |always `Infra` 4+| 
|template |see section above 4+| 
|metadata.name |name of the cluster 4+| 
|metadata.description |description of the cluster 4+| 
|spec.location |one of the locations that the cloud vendor provides | e.g. `West Europe` 3+| not available 
|spec.region |one of the regions that the cloud vendor provides | not available | e.g. `eu-central-1` | e.g. `de/fra` | e.g. `europe-central2`
|spec.k8sVersion |The K8s version (optional, defaults to whatever the cloud provider regards as the default) 4+| 
|spec.node_count |# of nodes in the cluster 4+|
|spec.vm_size |one of the VM sizes that the cloud vendor provides (optional) | e.g. `Standard_D2_v2` (which is also the default) 3+| not available
|spec.awsInstanceType |(optional) AWS EC2 instance type | not available | default: `t2.xlarge`, see links below 2+| not available
|spec.machineType |(optional) GKE machine type 3+| not available | default: `e2-standard-2`, see links below
|spec.numberOfCores |(optional) # of cores each node should have 2+| not available | default: `4` | not available
|spec.memoryMb |(optional) amount of memory each node should have in MB 2+| not available | default: `4096` | not available
|spec.diskType |(optional) type of disk each node should have 2+| not available | default: `SSD` | not available
|spec.diskSizeGb |(optional) size of the disk of the given node in GB 2+| not available | default: `250` | not available
|spec.versions |(optional) Map of versions of the Stackable operators to be used in this cluster. See below for a list of Stackable components. 4+|
|services |Map of service descriptions as embedded YAMLs. See below for available services. 4+|
|=======

==== Links

* https://aws.amazon.com/de/ec2/instance-types/[AWS EC2 instance types]
* https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html[AWS EC2 volume types]
* https://www.hetzner.com/de/cloud[Hetzner Cloud server types]
* https://cloud.google.com/compute/docs/machine-types[Google Cloud machine types]


=== Stackable operators

These are operators that Stackable currently provides. You can specify their versions with the `spec.versions` section in the cluster definition (see above).

[options="header"]
|=======
|Name |key
|https://github.com/stackabletech/commons-operator[Stackable Commons Operator] |`commons-operator`
|https://github.com/stackabletech/secret-operator[Stackable Secret Operator] |`secret-operator`
|https://github.com/stackabletech/airflow-operator[Stackable Operator for Apache Airflow] |`airflow-operator`
|https://github.com/stackabletech/druid-operator[Stackable Operator for Apache Druid] |`druid-operator`
|https://github.com/stackabletech/hbase-operator[Stackable Operator for Apache HBase] |`hbase-operator`
|https://github.com/stackabletech/hdfs-operator[Stackable Operator for Apache HDFS] |`hdfs-operator`
|https://github.com/stackabletech/hive-operator[Stackable Operator for Apache Hive] |`hive-operator`
|https://github.com/stackabletech/kafka-operator[Stackable Operator for Apache Kafka] |`kafka-operator`
|https://github.com/stackabletech/monitoring-operator[Stackable Operator for Monitoring and Metrics] |`monitoring-operator`
|https://github.com/stackabletech/nifi-operator[Stackable Operator for Apache NiFi] |`nifi-operator`
|https://github.com/stackabletech/opa-operator[Stackable Operator for OpenPolicyAgent (OPA)] |`opa-operator`
|https://github.com/stackabletech/spark-k8s-operator[Stackable Operator for Apache Spark] |`spark-k8s-operator`
|https://github.com/stackabletech/superset-operator[Stackable Operator for Apache Superset] |`superset-operator`
|https://github.com/stackabletech/trino-operator[Stackable Operator for Trino] |`trino-operator`
|https://github.com/stackabletech/zookeeper-operator[Stackable Operator for Apache ZooKeeper] |`zookeeper-operator`
|=======


==== Version literals

As shown in the example `cluster.yaml`, you can specify the versions of the Stackable components in the `spec.versions` section. The following table shows the different ways to do so by example:

[options="header"]
|=======
|example |description
|`RELEASE` | The newest release version which can be found in the Stackable repository
| (no version specified) | same as `RELEASE`
|`NIGHTLY` | The newest nightly version which can be found in the Stackable repository
|`NONE` | The operator is not installed at all.
|`0.2.0-mr404` | latest build of version 0.2.0 from GitHub Pull Request #404
|`0.3.0-nightly` | latest nightly build of version 0.3.0
|`0.6.1` | realeased version 0.6.1
|=======


==== Default version for Stackable Operators

To specify a version for *all* Stackable operators which are not explicitly mentioned in the Versions section, you can use the key `_-operator`. Using this operator most probably does not make sense with actual version numbers, but merely the keywords `RELEASE`, `NIGHTLY` or `NONE`.

If you'd like a cluster without any operators, you can set the version of `_-operator` to `NONE` as the only entry in the `versions` section.


=== Service descriptions

The service descriptions depend on the used services. Please refer to the documentation of the operator for the product. You find the links to the components in the table above.

== T2 Testdriver (T2 client Docker image)

We provide the Docker image `docker.stackable.tech/t2-testdriver` to make the usage of T2 in CI pipelines and for developers easier.

The T2 testdriver offers 3 "cluster modes" which are selected by setting the `CLUSTER` environment variable to either `NONE`, `EXISTING` or `MANAGED`. 

Independent of the "cluster mode", you can select if the testdriver is running "interactively" by setting the `INTERACTIVE_MODE` flag.

The following sections describe the meaning of the modes followed by a table describing all options.

=== Cluster mode NONE

The testdriver is not operating on a Kubernetes cluster at all. This mode is mostely useful for test and development purposes.

=== Cluster mode EXISTING

The testdriver operates on a cluster which exists independently from the testdriver. The testdriver neither creates nor terminates any cluster.

=== Cluster mode MANAGED

The testdriver creates a cluster as defined in the cluster definition file and tears it down once the testdriver is about to be shut down.

=== Interactive or not interactive?

The "normal" use case of the testdriver is the following: The testdriver executes the given test script against the (existing or managed) cluster, records the results and some other logfiles and then shuts down the cluster (if managed) and terminates.

If, on the other hand, started with `INTERACTIVE_MODE=true`, the testdriver does not execute a test script but waits after the creation of the cluster (if managed). You can then execute commands in the cluster as you wish. It might be useful to open a terminal session on the running container like this:

  docker exec -it <container_name> bash

Once you're done with the work, you should terminate the container running the `stop-session` command either from a terminal session like created above or directly by executing the command on the container.

  docker exec -it <container_name> stop-session

This way of terminating is preferred to just terminating the container because the grace period of `docker stop` usually is too short to allow for an unproblematic cluster shutdown.

=== Options

The following table describes all the options that can/must be set when using the testdriver.

[options="header"]
|=======
|Feature |How to use |Description
|Cluster mode | environment variable `CLUSTER` (`NONE`, `EXISTING` or `MANAGED`) | *(mandatory)* See sections above...
|Interactive mode | environment variable `INTERACTIVE_MODE` | *(optional)*, defaults to `false`
|User/Group for target directory | environment variable `UID_GID` (format `<UID>:<GID>`), defaults to `0:0` (root) | *(optional)* All stuff which is written into the target dir is owned by this user/group combination.
|T2 URL | environment variable `T2_URL` | *(mandatory for managed clusters)* The URL of T2 to use
|T2 Token | environment variable `T2_TOKEN` | *(mandatory for managed clusters)* The token to access T2
|Cluster definition | map as file to `/cluster.yaml` | *(mandatory for managed clusters)* The cluster definition as described above
|Target directory | map as volume to `/target/` | *(mandatory)* The target directory for the output
|Kubernetes config file | map as file to `/root/.kube/config` | *(mandatory if using existing clusters)* The K8s config file to access the existing cluster
|Test script | map as file to `/test.sh` | *(mandatory if not running in interactive mode)*. The script containing the test to be run once the cluster is up and running
|=======

=== Output files

The following files are created in the directory mounted into `/target/`:

[options="header"]
|=======
|File |Description
|`testdriver.log` | Log file of the testdriver container itself
|`k8s-event.log` | Event stream of the Kubernetes cluster (one YAML per event)
|`k8s-event-short.log` | Event stream of the Kubernetes cluster (one line per event)
|`k8s-pod-change.log` | Pod changes stream of the Kubernetes cluster (one YAML per change)
|`k8s-pod-change-short.log` | Pod changes stream of the Kubernetes cluster (one line per change)
|`k8s-ping.log` | The testdriver "pings" (`kubectl get pods ...`) the (existing or managed) cluster every 5 seconds to document/test its connectivity. This file contains the results of these pings
|`k8s-summary.log` | The summary of the K8s "pings" mentioned above (counts grouped by result type)
|`stackable-versions.txt` | Text file containing the versions of the installed Stackable components in the cluster (if managed)
|`test-output.log` | Output of the test script
|=======

=== Return code

* If the T2 testdriver is not able to create or tear down the cluster, it returns `255`.
* Otherwise, the return code of the Docker container process is the return code of the test script which was injected into it.


