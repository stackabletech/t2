// Header of this document:

= Using T2
:toc:
:toc-placement: preamble
:toclevels: 3
:showtitle:
:base-repo: https://github.com/stackabletech/t2
:imagesdir: diagrams

// Need some preamble to get TOC:
{empty}

At Stackable, we have a https://t2.stackable.tech/swagger-ui/[running instance of T2, window="_blank"] to provision our own clusters. As described in the link:../README.adoc[README], we use it for integration testing and troubleshooting and do not provide access publicly (except DIY, see below).

== Create a cluster

=== Create a cluster in T2's infrastructure

With request to https://t2.stackable.tech/swagger-ui/#/cluster-controller/createClusterUsingPOST[this endpoint, window="_blank"] you can create a new cluster. You have to provide the Stackable cluster definition as payload and a Token via the `t2-token` HTTP header to authenticate. The response is a description of the current cluster state.

To track the progress while the cluster is created, you can use https://t2.stackable.tech/swagger-ui/#/cluster-controller/getClusterUsingGET[this endpoint, window="_blank"]. The desired state you want to wait for is `RUNNING`. (requires token as well)

To see in more detail what T2 is doing to create your cluster, you can https://t2.stackable.tech/swagger-ui/#/cluster-controller/getLogUsingGET[trace the output log here, window="_blank"]. (requires token as well)

Once the cluster is up and running, you can download a script https://t2.stackable.tech/swagger-ui/#/cluster-controller/getClientScriptUsingGET[here, window="_blank"] which provides you with a convenient way to access the nodes of the cluster (see <<client_script>> below).

To check which versions of the Stackable components (agent, operators) are running in your cluster, you can get an overview https://t2.stackable.tech/swagger-ui/#/cluster-controller/getLogUsingGET[here, window="_blank"]. (requires token as well)

==== VPN (Wireguard)

A very convenient way to work with a running cluster is to put your machine in the provided VPN. Using http://t2.stackable.tech/swagger-ui/#/cluster-controller/getWireguardConfigUsingGET[this endpoint, window="_blank"] you can download a Wireguard config file which is ready to be used. Please refer to the Wireguard docs for your system to find out how to use this.

Once you are in the same network with the cluster, you can http://t2.stackable.tech/swagger-ui/#/cluster-controller/getKubeconfigUsingGET[download a kubeconfig file, window="_blank"] to interact with the Kubernetes system via *kubectl*, *k9s* or the like.



=== Create a cluster in your infrastructure

To create a cluster yourself, we offer what we call the *DIY option*. To use it, you can use https://t2.stackable.tech/swagger-ui/#/diy-cluster-controller/createClusterUsingGET[this service]. You have to provide a Stackable cluster definition as request body and get a ZIP file in return. This ZIP file comes with a `readme.txt` which explains the usage.


[[client_script]]
== The Stackable client script

This script can be used to access a running cluster conveniently. 

The script expects the private SSH key (matching one of the public keys in the Stackable cluster definition, see <<yaml>>) to be in your keystore (`~/.ssh/` in Linux). If you keep it at another location, you can provide the path to the private key with the `-i` option.

To ssh into a host, just provide the hostname as the single parameter, e.g.

[source,bash]
----
./stackable.sh worker-12
----

or, with path to key: 

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key
----

If you want to execute a command on the host, you can add it as a second param, e.g.

[source,bash]
----
./stackable.sh worker-12 "kubectl get nodes"
----

or, with path to key:

[source,bash]
----
./stackable.sh worker-12 -i path/to/my/key "kubectl get nodes"
----

[[yaml]]
== The Stackable cluster definition YAML

In either way you choose to use T2, you have to provide a definition of the Stackable cluster you want to create. You do this in a single YAML file. This section describes the cluster definition.

This is an example cluster definition, please find detailed specification of the structure below.

[source,yaml]
----
apiVersion: t2.stackable.tech/v1
kind: Infra
template: demo-debian-10
metadata: 
  name: stackable-demo
  description: "This is the cluster I want!"
domain: stackable.demo
publicKeys:
  - "ssh-rsa AAAAB3NzaC1..."
  - "ssh-rsa AAAACmtp4Ko..."
spec:
  region: de/fra
  cpuFamily: INTEL_XEON
  wireguard: true
  versions:
    stackable-agent: "1.2.3"
    stackable-spark-operator-server: "2.0.0"
    stackable-kafka-operator-server: "0.0.1-alpha"
  orchestrator:
    numberOfCores: 4
    memoryMb: 8192
    diskType: HDD
    diskSizeGb: 15
  nodes:
    main:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    worker:
      numberOfNodes: 4
      numberOfCores: 4
      memoryMb: 8192
      diskType: HDD 
      diskSizeGb: 15
    testdriver:
      numberOfNodes: 1
      numberOfCores: 2
      memoryMb: 2048
      diskType: HDD 
      diskSizeGb: 15
      agent: false
services:
  spark-primary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-primary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9999
            masterWebUiPort: 11111
      worker:
        selectors:
          - nodeName: "worker-1.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-3.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0
  spark-secondary: |
    apiVersion: spark.stackable.tech/v1
    kind: SparkCluster
    metadata:
      name: spark-secondary
    spec:
      master:
        selectors:
          - nodeName: "main-1.stackable.demo"
            instances: 1
            masterPort: 9998
            masterWebUiPort: 11112
      worker:
        selectors:
          - nodeName: "worker-2.stackable.demo"
            instances: 1
            cores: 1
            memory: "1g"
      historyServer:
        selectors:
          - nodeName: "worker-4.stackable.demo"
            instances: 1
      version: "3.0.1"
      maxPortRetries: 0        
----

The following sections describe the fields of the cluster definition in more detail:

=== Templates

The most important choice you have to make in a cluster definition is right in line #3: the template! By choosing a template, you decide which cloud provider will host your cluster and which Linux distribution will be used.

The following table lists the currently available templates:

[options="header"]
|=======
|key |Cloud provider| Linux distribution
|demo-centos-7 | IONOS cloud | CentOS 7
|demo-centos-8 | IONOS cloud | CentOS 8
|demo-debian-10 | IONOS cloud | Debian 10
|aws-centos-8 | Amazon EC2 | CentOS 8
|=======

=== Configuration options

[options="header"]
|=======
|key |description |IONOS |AWS EC2
|apiVersion |always `t2.stackable.tech/v1` 2+| 
|kind |always `Infra` 2+| 
|template |see section above 2+| 
|metadata.name |name of the cluster 2+| 
|metadata.description |description of the cluster 2+| 
|domain |domain for DNS inside the cluster or when accessing through VPN 2+| 
|publicKeys |list of SSH public keys to allow access to cluster nodes 2+| 
|spec.region |one of the regions that the cloud vendor provides | e.g. `de/fra`, `de/txl` | e.g. `eu-central-1`
|spec.cpuFamily |(optional) specify CPU-Family for all servers. The allowed values depend on the datacenter location you set up your cluster in. Please refer to your IONOS account for information about available CPUs and default values. | e.g. `INTEL_XEON` | not available
|spec.wireguard |(boolean, optional, defaults to `true`) Should a wireguard server be started on the bastion host? Switching wireguard off when you don't need it can speed up the start of the cluster. 2+|
|spec.versions |(optional) Map of versions of the Stackable components to be used in this cluster. See below for a list of Stackable components. 2+|
|spec.orchestrator |(optional) The orchestrator node is the Stackable node which hosts the operators. It is required, you cannot opt out of having one. It has reasonable defaults, but you can overwrite them with the config properties in this section. Be cautious not to configure an orchestrator which has too little power. See following entries for details. 2+|
|spec.orchestrator.numberOfCores |(optional) # of cores the orchestrator should have | default: `4` | not available
|spec.orchestrator.memoryMb |(optional) amount of memory the orchestrator should have in MB | default: `8192` | not available
|spec.orchestrator.diskType |(optional) type of disk the orchestrator should have | default: `HDD` | default: `gp2` (general purpose SSD) 
|spec.orchestrator.diskSizeGb |(optional) size of the disk of the orchestrator in GB | default: `50` | default: `50`
|spec.orchestrator.aws_instance_type |(optional) AWS EC2 instance type | not available | default: `t2.xlarge`
|spec.nodes |map of node types with their specification 2+|
|spec.nodes.<type>.numberOfNodes |# of nodes of the given type 2+|
|spec.nodes.<type>.numberOfCores |# of cores each node of the given type should have | default: `4` | not available
|spec.nodes.<type>.memoryMb |amount of memory each node of the given type should have in MB | default: `8192` | not available
|spec.nodes.<type>.diskType | type of disk each node of the given type should have | default: `HDD` | default: `gp2` (general purpose SSD) 
|spec.nodes.<type>.diskSizeGb |size of the disk of the given node in GB | default: `50` | default: `50`
|spec.nodes.<type>.aws_instance_type |(optional) AWS EC2 instance type | not available | default: `t2.medium`
|spec.nodes.<type>.agent |(boolean, optional, defaults to `true`) Should a Stackable agent be run on this node? 2+|
|services |Map of service descriptions as embedded YAMLs. See below for available services. 2+|
|=======

==== Links

* https://aws.amazon.com/de/ec2/instance-types/[AWS EC2 instance types]

=== Stackable components

These are components that a Stackable cluster is made of. You can specify their versions with the `spec.versions` section in the cluster definition (see above)

[options="header"]
|=======
|Name |key
|https://github.com/stackabletech/agent[Stackable Agent] | `stackable-agent`
|https://github.com/stackabletech/spark-operator[Stackable Spark Operator] |`stackable-spark-operator-server`
|https://github.com/stackabletech/zookeeper-operator[Stackable Zookeeper Operator] |`stackable-zookeeper-operator-server`
|https://github.com/stackabletech/kafka-operator[Stackable Kafka Operator] |`stackable-kafka-operator-server`
|https://github.com/stackabletech/nifi-operator[Stackable NiFi Operator] |`stackable-nifi-operator-server`
|https://github.com/stackabletech/opa-operator[Stackable OPA Operator] |`stackable-nifi-operator-server`
|https://github.com/stackabletech/regorule-operator[Stackable Regorule Operator] |`stackable-nifi-operator-server`
|=======


=== Service descriptions

The service descriptions depend on the used services. Please refer to the documentation of the operator for the product: 

* https://github.com/stackabletech/spark-operator[Apache Spark]
* https://github.com/stackabletech/zookeeper-operator[Apache ZooKeeper]
* https://github.com/stackabletech/kafka-operator[Apache Kafka]
* https://github.com/stackabletech/nifi-operator[Apache NiFi]
* https://github.com/stackabletech/opa-operator[Open Policy Agent (OPA)]
* https://github.com/stackabletech/regorule-operator[Regorule (Policies for OPA)]

== The T2 client script

If you want to automate your Stackable cluster generation (e.g. in a CI/CD pipeline), you can use a https://raw.githubusercontent.com/stackabletech/t2/client-script/client/t2.py[Python script^] that we provide with T2. This section describes the usage of this script.

=== How the script works

Roughly, this is what the script does:

* Launch
** Create a folder `.cluster/` where the temporary files go
** Generate an SSH keypair
** Make a copy of your provided cluster definition file and add the public key to the `publicKeys` section. (If you do not plan to provide any SSH key on your own, please provide that section as an empty list!)
** Call T2 to launch a new cluster
** Wait until the cluster is up and running
** Download the Stackable client script (see <<client_script>>) into your current folder to easily access the built cluster.
* Terminate
** Call T2 to tear the cluster down
** Wait until the cluster is terminated

=== The launch command

The `launch` command needs as params:

. the T2 token to authenticate
. the base URL of the T2 REST API
. the path to a valid cluster definition file

Example:

[source,bash]
----
python3 t2.py launch my-secret-token https://t2.stackable.tech path/to/my/cluster.yaml
----


=== The terminate command

The `terminate` command needs as params:

. the T2 token to authenticate
. the base URL of the T2 REST API

Example:

[source,bash]
----
python3 t2.py terminate my-secret-token https://t2.stackable.tech 
----

